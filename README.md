# AI_security_papers_17_21

AI security 2017-2021(if any) 信息安全四大相关papers 分类 约100篇

格式为：[会议+年份] 论文 (领域or数据集)

# Membership inference attacks and model inversion attacks

[USENIX18] AttriGuard- A Practical Defense Against Attribute Inference Attacks via Adversarial Machine Learning. (Google apps dataset)

[USENIX20] Stolen Memories- Leveraging Model Memorization for Calibrated White-Box Membership Inference. (image)

[USENIX20] Updates-Leak- Data Set Inference and Reconstruction Attacks in Online Learning. (image)

[USENIX21] Stealing Links from Graph Neural Networks. (GNN)
[USENIX21] Systematic Evaluation of Privacy Risks of Machine Learning Models.

[S&P17] Membership Inference Attacks Against Machine Learning Models.

[S&P19] Comprehensive Privacy Analysis of Deep Learning. (CIFAR)

[S&P19] Exploiting Unintended Feature Leakage in Collaborative Learning. 

[S&P20] Privacy Risks of General-Purpose Language Models. (NLP)

[S&P21] Machine Unlearning.

[NDSS19] MBeacon- Privacy-Preserving Beacons for DNA Methylation Data.

[NDSS19] ML-Leaks- Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models. (image)

[NDSS21] Practical Blind Membership Inference Attack via Differential Comparisons.

[CCS17] Machine Learning Models that Remember Too Much.

[CCS18] Machine Learning with Membership Privacy using Adversarial Regularization.

[CCS18] Property Inference Attacks on Fully Connected Neural Networks using Permutation Invariant Representations. 

[CCS19] MemGuard- Defending against Black-Box Membership Inference Attacks via Adversarial Examples.

[CCS19] Neural Network Inversion in Adversarial Setting via Background Knowledge Alignment.

[CCS19] Privacy Risks of Securing Machine Learning Models against Adversarial Examples.

[CCS20] Analyzing Information Leakage of Updates to Natural Language Models.

[CCS20] GAN-Leaks- A Taxonomy of Membership Inference Attacks against Generative Models.


# adversarial examples

[USENIX18] CommanderSong- A Systematic Approach for Practical Adversarial Voice Recognition. (speech)

[USENIX18] Formal Security Analysis of Neural Networks using Symbolic Intervals. (ACAS Xu, MNIST)

[USENIX19] Misleading Authorship Attribution of Source Code using Adversarial Learning. (code)

[USENIX20] Devil’s Whisper- A General Approach for Physical Adversarial Attacks against Commercial Black-box Speech Recognition Devices. (speech)

[USENIX20] Hybrid Batch Attacks- Finding Black-box Adversarial Examples with Limited Queries. (MNIST, CIFAR10, ImageNet)

[USENIX20] Interpretable Deep Learning under Fire. (defense)

[USENIX20] TEXTSHIELD- Robust Text Classification Based on Multimodal Embedding and Neural Machine Translation. (text, defense)

[USENIX21] Defeating DNN-Based Traffic Analysis Systems in Real-Time With Blind Adversarial Perturbations. (network traffic)

[USENIX21] SLAP- Improving Physical Adversarial Examples with Short-Lived Adversarial Perturbations.

[USENIX21] WaveGuard- Understanding and Mitigating Audio Adversarial Examples. (speech)

[S&P17] Towards Evaluating the Robustness of Neural Networks. (image)

[S&P18] AI2 - Safety and Robustness Certification of Neural Networks with Abstract Interpretation. (defense)

[S&P19] Certified Robustness to Adversarial Examples with Differential Privacy. (defense)

[S&P19] DEEPSEC- A Uniform Platform for Security Analysis of Deep Learning Model.

[S&P20] HopSkipJumpAttack- A Query-Efficient Decision-Based Attack. (image)

[S&P21] Hear "No Evil", See "Kenansville"- Efficient and Transferable Black-Box Attacks on Speech Recognition and Voice Identification Systems. (speech)

[S&P21] Who is Real Bob? Adversarial Attacks on Speaker Recognition Systems. (speech)

[NDSS18] Feature Squeezing- Detecting Adversarial Examples in Deep Neural Networks

[NDSS19] Adversarial Attacks Against Automatic Speech Recognition Systems via Psychoacoustic Hiding. (speech)

[NDSS19] NIC- Detecting Adversarial Samples with Neural Network Invariant Checking. (image)

[NDSS19] Practical Hidden Voice Attacks against Speech and Speaker Recognition Systems. (speech)

[NDSS19] Stealthy Adversarial Perturbations Against Real-Time Video Classification Systems. (video)

[NDSS19] TEXTBUGGER- Generating Adversarial Text Against Real-world Applications. (text)

[CCS17] DolphinAttack- Inaudible Voice Commands. (voice)

[CCS17] MagNet- a Two-Pronged Defense against Adversarial Examples.

[CCS19] Procedural Noise Adversarial Examples for Black-Box Attacks on Deep Convolutional Networks.

[CCS19] Seeing isn’t Believing- Towards More Robust Adversarial Attack Against Real World Object Detectors.

[CCS20] DeepDyve- Dynamic Verification for Deep Neural Networks.

[CCS20] Goa Catch ’Em All- Using Honeypots to Catch Adversarial Aacks on Neural Networks.


# Backdoor

[USENIX18] Turning Your Weakness Into a Strength- Watermarking Deep Neural Networks by Backdooring. (image)

[USENIX21] Demon in the Variant- Statistical Analysis of DNNs for Robust Backdoor Contamination Detection. (image)

[USENIX21] Explanation-Guided Backdoor Poisoning Attacks Against Malware Classifiers.

[USENIX21] Graph Backdoor. (GNN)

[USENIX21] T-Miner - A Generative Approach to Defend Against Trojan Attacks on DNN-based Text Classification. (text, defense)

[S&P19] Neural Cleanse- Identifying and Mitigating Backdoor Attacks in Neural Networks. (defense, image)

[S&P21] Detecting AI Trojans Using Meta Neural Analysis.

[NDSS18] Trojaning Attack on Neural Networks.

[CCS19] ABS- Scanning Neural Networks for Back-doors by Artificial Brain Stimulation. (defense)

[CCS19] Latent Backdoor Attacks on Deep Neural Networks.

[CCS20] Composite Backdoor Attack for Deep Neural Network by Mixing Existing Benign Features



# Poisoning attacks

[USENIX18] When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks.

[USENIX19] Seeing is Not Believing- Camouflage Attacks on Image Scaling Algorithms. (image)

[USENIX19] Why Do Adversarial Attacks Transfer? Explaining Transferability of Evasion and Poisoning Attacks.

[USENIX20] Adversarial Preprocessing- Understanding and Preventing Image-Scaling Attacks in Machine Learning. (image)

[S&P18] Manipulating Machine Learning- Poisoning Attacks and Countermeasures for Regression Learning.

[S&P20] Humpty Dumpty- Controlling Word Meanings via Corpus Poisoning. (NLP)

[NDSS21] Data Poisoning Attacks to Deep Learning Based Recommender Systems. 

[CCS18] Model-Reuse Attacks on Deep Learning Systems.

[CCS20] A Tale of Evil Twins- Adversarial Inputs versus Poisoned Models.


# Evasion attacks

[USENIX19] Improving Robustness of ML Classifiers against Realizable Evasion Attacks Using Conserved Features. (PDF malware detection)

[S&P20] Intriguing Properties of Adversarial ML Attacks in the Problem Space.

[NDSS20] When Malware is Packin’ Heat; Limits of Machine Learning Classifiers Based on Static Analysis Features. (defense)

[CCS17] Evading Classifiers by Morphing in the Dark.

[CCS19] AdVersarial- Perceptual Ad Blocking meets Adversarial Machine Learning.


# Model extraction attacks

[USENIX20] Exploring Connections Between Active Learning and Model Extraction.

[USENIX20] High Accuracy and High Fidelity Extraction of Neural Networks. (image)

[USENIX21] DRMI- A Dataset Reduction Technology based on Mutual Information for Black-box Attacks. (image)

[S&P18] Stealing Hyperparameters in Machine Learning.

[NDSS20] CloudLeak- Large-Scale Deep Learning Models Stealing Through Adversarial Examples. (image)


# Distributed Learning or Federated Learning

[USENIX20] Justinian’s GAAvernor- Robust Distributed Learning with Gradient Aggregation Agent.

[USENIX20] Local Model Poisoning Attacks to Byzantine-Robust Federated Learning.

[S&P19] Helen- Maliciously Secure Coopetitive Learning for Linear Models.

[NDSS21] FLTrust- Byzantine-robust Federated Learning via Trust Bootstrapping.

[NDSS21] Manipulating the Byzantine- Optimizing Model Poisoning Attacks and Defenses for Federated Learning.

[NDSS21] POSEIDON- Privacy-Preserving Federated Neural Network Learning.

[CCS17] Deep Models Under the GAN- Information Leakage from Collaborative Deep Learning.



# Others

[USENIX18] With Great Training Comes Great Vulnerability- Practical Attacks against Transfer Learning.

[USENIX20] Fawkes- Protecting Privacy against Unauthorized Deep Learning Models. (image)

[USENIX21] Adversarial Policy Training against Deep Reinforcement Learning.

[S&P21] Adversarial Watermarking Transformer- Towards Tracing Text Provenance with Data Hiding.

[S&P21] Poltergeist- Acoustic Adversarial Machine Learning against Cameras and Computer Vision.

[S&P21] SoK- The Faults in our ASRs- An Overview of Attacks against Automatic Speech Recognition and Speaker Identification Systems. (Overview)

[NDSS19] Life after Speech Recognition- Fuzzing Semantic Misinterpretation for Voice Assistant Applications. (speech)

[NDSS21] FARE- Enabling Fine-grained Attack Categorization under Low-quality Labeled Data.

[CCS17] Practical Attacks Against Graph-based Clustering.

[CCS18] LEMNA- Explaining Deep Learning based Security Applications.


